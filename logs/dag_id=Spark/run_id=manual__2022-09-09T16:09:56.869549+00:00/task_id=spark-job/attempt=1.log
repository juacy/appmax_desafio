[2022-09-09T16:10:01.505+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: Spark.spark-job manual__2022-09-09T16:09:56.869549+00:00 [queued]>
[2022-09-09T16:10:01.513+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: Spark.spark-job manual__2022-09-09T16:09:56.869549+00:00 [queued]>
[2022-09-09T16:10:01.513+0000] {taskinstance.py:1368} INFO - 
--------------------------------------------------------------------------------
[2022-09-09T16:10:01.513+0000] {taskinstance.py:1369} INFO - Starting attempt 1 of 1
[2022-09-09T16:10:01.513+0000] {taskinstance.py:1370} INFO - 
--------------------------------------------------------------------------------
[2022-09-09T16:10:01.750+0000] {taskinstance.py:1389} INFO - Executing <Task(SparkSubmitOperator): spark-job> on 2022-09-09 16:09:56.869549+00:00
[2022-09-09T16:10:01.755+0000] {standard_task_runner.py:52} INFO - Started process 101 to run task
[2022-09-09T16:10:01.758+0000] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'Spark', 'spark-job', 'manual__2022-09-09T16:09:56.869549+00:00', '--job-id', '21', '--raw', '--subdir', 'DAGS_FOLDER/teste_spark.py', '--cfg-path', '/tmp/tmpjuyk22ay', '--error-file', '/tmp/tmpd_980kb0']
[2022-09-09T16:10:01.759+0000] {standard_task_runner.py:80} INFO - Job 21: Subtask spark-job
[2022-09-09T16:10:02.112+0000] {task_command.py:371} INFO - Running <TaskInstance: Spark.spark-job manual__2022-09-09T16:09:56.869549+00:00 [running]> on host dbdbd3f5e7fc
[2022-09-09T16:10:02.364+0000] {taskinstance.py:1583} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=juacy
AIRFLOW_CTX_DAG_ID=Spark
AIRFLOW_CTX_TASK_ID=spark-job
AIRFLOW_CTX_EXECUTION_DATE=2022-09-09T16:09:56.869549+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-09-09T16:09:56.869549+00:00
[2022-09-09T16:10:02.371+0000] {base.py:68} INFO - Using connection ID 'conexao_spark' for task execution.
[2022-09-09T16:10:02.372+0000] {spark_submit.py:335} INFO - Spark-Submit cmd: spark-submit --master spark://fe77514119f4:7077 --name arrow-spark ./dags/spark-app.py
[2022-09-09T16:10:04.867+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:04 INFO SparkContext: Running Spark version 3.3.0
[2022-09-09T16:10:04.918+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-09-09T16:10:05.000+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO ResourceUtils: ==============================================================
[2022-09-09T16:10:05.000+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-09-09T16:10:05.000+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO ResourceUtils: ==============================================================
[2022-09-09T16:10:05.001+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO SparkContext: Submitted application: First App
[2022-09-09T16:10:05.019+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-09-09T16:10:05.031+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO ResourceProfile: Limiting resource is cpu
[2022-09-09T16:10:05.031+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-09-09T16:10:05.075+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO SecurityManager: Changing view acls to: default
[2022-09-09T16:10:05.076+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO SecurityManager: Changing modify acls to: default
[2022-09-09T16:10:05.076+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO SecurityManager: Changing view acls groups to:
[2022-09-09T16:10:05.077+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO SecurityManager: Changing modify acls groups to:
[2022-09-09T16:10:05.077+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(default); groups with view permissions: Set(); users  with modify permissions: Set(default); groups with modify permissions: Set()
[2022-09-09T16:10:05.292+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO Utils: Successfully started service 'sparkDriver' on port 38753.
[2022-09-09T16:10:05.364+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO SparkEnv: Registering MapOutputTracker
[2022-09-09T16:10:05.397+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO SparkEnv: Registering BlockManagerMaster
[2022-09-09T16:10:05.412+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-09-09T16:10:05.413+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-09-09T16:10:05.416+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-09-09T16:10:05.438+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6f606cee-a4ef-4e12-a878-c18d575ff6f7
[2022-09-09T16:10:05.456+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-09-09T16:10:05.484+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-09-09T16:10:05.677+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-09-09T16:10:05.764+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO Executor: Starting executor ID driver on host dbdbd3f5e7fc
[2022-09-09T16:10:05.770+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2022-09-09T16:10:05.787+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44511.
[2022-09-09T16:10:05.787+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO NettyBlockTransferService: Server created on dbdbd3f5e7fc:44511
[2022-09-09T16:10:05.789+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-09-09T16:10:05.794+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dbdbd3f5e7fc, 44511, None)
[2022-09-09T16:10:05.797+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO BlockManagerMasterEndpoint: Registering block manager dbdbd3f5e7fc:44511 with 434.4 MiB RAM, BlockManagerId(driver, dbdbd3f5e7fc, 44511, None)
[2022-09-09T16:10:05.799+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dbdbd3f5e7fc, 44511, None)
[2022-09-09T16:10:05.800+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, dbdbd3f5e7fc, 44511, None)
[2022-09-09T16:10:05.995+0000] {spark_submit.py:488} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.
[2022-09-09T16:10:06.995+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-09-09T16:10:07.001+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:07 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2022-09-09T16:10:09.022+0000] {spark_submit.py:488} INFO - parte 1
[2022-09-09T16:10:09.443+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-09-09T16:10:09.443+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-09-09T16:10:09.444+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-09-09T16:10:09.637+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO CodeGenerator: Code generated in 124.5017 ms
[2022-09-09T16:10:09.736+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2022-09-09T16:10:09.747+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-09-09T16:10:09.747+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2022-09-09T16:10:09.747+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO DAGScheduler: Parents of final stage: List()
[2022-09-09T16:10:09.748+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO DAGScheduler: Missing parents: List()
[2022-09-09T16:10:09.751+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-09-09T16:10:09.833+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 213.2 KiB, free 434.2 MiB)
[2022-09-09T16:10:09.856+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 77.4 KiB, free 434.1 MiB)
[2022-09-09T16:10:09.858+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on dbdbd3f5e7fc:44511 (size: 77.4 KiB, free: 434.3 MiB)
[2022-09-09T16:10:09.861+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513
[2022-09-09T16:10:09.872+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-09-09T16:10:09.873+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-09-09T16:10:09.913+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (dbdbd3f5e7fc, executor driver, partition 0, PROCESS_LOCAL, 4535 bytes) taskResourceAssignments Map()
[2022-09-09T16:10:09.924+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-09-09T16:10:10.742+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-09-09T16:10:10.742+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-09-09T16:10:10.743+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-09-09T16:10:10.806+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO PythonRunner: Times: total = 696, boot = 689, init = 7, finish = 0
[2022-09-09T16:10:10.814+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO FileOutputCommitter: Saved output of task 'attempt_202209091610092108962716437875437_0000_m_000000_0' to file:/opt/***/dados/bronze/teste.csv/_temporary/0/task_202209091610092108962716437875437_0000_m_000000
[2022-09-09T16:10:10.814+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO SparkHadoopMapRedUtil: attempt_202209091610092108962716437875437_0000_m_000000_0: Committed. Elapsed time: 1 ms.
[2022-09-09T16:10:10.829+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2914 bytes result sent to driver
[2022-09-09T16:10:10.835+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 930 ms on dbdbd3f5e7fc (executor driver) (1/1)
[2022-09-09T16:10:10.837+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-09-09T16:10:10.840+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 44111
[2022-09-09T16:10:10.844+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.083 s
[2022-09-09T16:10:10.847+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-09-09T16:10:10.847+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-09-09T16:10:10.849+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.112859 s
[2022-09-09T16:10:10.851+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO FileFormatWriter: Start to commit write Job 8f7c10bb-327e-452b-9682-55fdc80824e3.
[2022-09-09T16:10:10.864+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO FileFormatWriter: Write Job 8f7c10bb-327e-452b-9682-55fdc80824e3 committed. Elapsed time: 12 ms.
[2022-09-09T16:10:10.868+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO FileFormatWriter: Finished processing stats for write job 8f7c10bb-327e-452b-9682-55fdc80824e3.
[2022-09-09T16:10:10.876+0000] {spark_submit.py:488} INFO - root
[2022-09-09T16:10:10.877+0000] {spark_submit.py:488} INFO - |-- Category: string (nullable = true)
[2022-09-09T16:10:10.877+0000] {spark_submit.py:488} INFO - |-- ID: long (nullable = true)
[2022-09-09T16:10:10.877+0000] {spark_submit.py:488} INFO - |-- Truth: boolean (nullable = true)
[2022-09-09T16:10:10.877+0000] {spark_submit.py:488} INFO - |-- Value: double (nullable = true)
[2022-09-09T16:10:10.877+0000] {spark_submit.py:488} INFO - 
[2022-09-09T16:10:10.877+0000] {spark_submit.py:488} INFO - parte 2
[2022-09-09T16:10:10.953+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO SparkContext: Invoking stop() from shutdown hook
[2022-09-09T16:10:10.963+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO SparkUI: Stopped Spark web UI at http://dbdbd3f5e7fc:4040
[2022-09-09T16:10:10.976+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-09-09T16:10:10.986+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO MemoryStore: MemoryStore cleared
[2022-09-09T16:10:10.986+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO BlockManager: BlockManager stopped
[2022-09-09T16:10:10.993+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-09-09T16:10:10.995+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-09-09T16:10:10.999+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO SparkContext: Successfully stopped SparkContext
[2022-09-09T16:10:11.000+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:10 INFO ShutdownHookManager: Shutdown hook called
[2022-09-09T16:10:11.000+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-7cfc1e52-8f21-4ea1-8ce1-5ff980b491cb
[2022-09-09T16:10:11.003+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-363b9e8a-a20c-468d-b855-22dc8d866267
[2022-09-09T16:10:11.006+0000] {spark_submit.py:488} INFO - 22/09/09 16:10:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-7cfc1e52-8f21-4ea1-8ce1-5ff980b491cb/pyspark-fb1c5c6e-d3e1-47b9-ae20-e12f5b7fa994
[2022-09-09T16:10:11.042+0000] {taskinstance.py:1412} INFO - Marking task as SUCCESS. dag_id=Spark, task_id=spark-job, execution_date=20220909T160956, start_date=20220909T161001, end_date=20220909T161011
[2022-09-09T16:10:11.244+0000] {local_task_job.py:156} INFO - Task exited with return code 0
[2022-09-09T16:10:11.292+0000] {local_task_job.py:279} INFO - 0 downstream tasks scheduled from follow-on schedule check
